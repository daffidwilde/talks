{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<h1 style='font-size:60px'>\n", "    Parallelisation in Python\n", "</h1>\n", "\n", "<font size='5'> Henry Wilde | \n", "<i class='fa fa-github' aria-hidden='false'></i>\n", "<i class='fa fa-twitter' aria-hidden='false'></i> @daffidwilde </font>\n", "<hr>\n", "\n", "# Classically used for:\n", "\n", "<br>\n", "<img src=\"img/simple.pdf\" width=\"700\">\n", "<br><br>\n", "\n", "# Can be used for:\n", "\n", "<br>\n", "<img src=\"img/complex.pdf\" width=\"800\">\n", "<br><br>\n", "\n", "---\n", "\n", "# Three approaches:\n", "\n", "- First principles with `multiprocessing`\n", "- The \"standard\" approach with `multiprocessing`\n", "- A new approach with `dask` (documentation: [docs.dask.org](http://docs.dask.org/en/latest/))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import ciw\n", "\n", "\n", "def run_simulation(num_servers, seed):\n", "    \"\"\"\n", "    Simulate a simple M|M|c queue for 10,000 time units and find the mean waiting time.\n", "    \"\"\"\n", "\n", "    ciw.seed(seed)\n", "\n", "    N = ciw.create_network(\n", "        Arrival_distributions=[[\"Exponential\", 0.2]],\n", "        Service_distributions=[[\"Exponential\", 0.1]],\n", "        Number_of_servers=[num_servers],\n", "    )\n", "\n", "    Q = ciw.Simulation(N)\n", "    Q.simulate_until_max_time(10000)\n", "\n", "    recs = Q.get_all_records()\n", "    waits = [rec.waiting_time for rec in recs]\n", "\n", "    return num_servers, sum(waits) / len(waits)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["run_simulation(2, 0)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "# Doing things sequentially"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from collections import defaultdict\n", "import itertools\n", "\n", "numbers_of_servers = [1, 2, 3]\n", "trials = range(100)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "results = defaultdict(list)\n", "for servers, trial in itertools.product(numbers_of_servers, trials):\n", "    servers, time = run_simulation(servers, trial)\n", "    results[servers].append(time)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "# First principles (queues, queues, queues)\n", "\n", "![In and out trays](img/punch-in-out.jpg)\n", "Image via: Punch magazine"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def put_down_tasks(work_queue, numbers_of_servers, trials):\n", "    \"\"\" Put each of the task's arguments into the work queue. \"\"\"\n", "\n", "    for servers, trial in itertools.product(numbers_of_servers, trials):\n", "        work_queue.put((servers, trial))\n", "\n", "    print(\"Tasks sent out.\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def worker(work_queue, done_queue):\n", "    \"\"\" Tell the worker to grab a job, execute it and then stop. \"\"\"\n", "\n", "    for servers, trial in iter(work_queue.get, \"STOP\"):\n", "        result = run_simulation(servers, trial)\n", "        done_queue.put(result)\n", "\n", "    done_queue.put(\"STOP\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def start_workers(work_queue, done_queue, workers):\n", "    \"\"\" Set the workers going as their own process. \"\"\"\n", "\n", "    for _ in range(workers):\n", "        process = mp.Process(target=worker, args=(work_queue, done_queue))\n", "        work_queue.put(\"STOP\")\n", "        process.start()\n", "\n", "    print(\"Workers started.\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def process_done_queue(done_queue, workers):\n", "    \"\"\" Retrieve the average waiting times. \"\"\"\n", "\n", "    stops = 0\n", "    results = defaultdict(list)\n", "    while stops < workers:\n", "        result = done_queue.get()\n", "        if result == \"STOP\":\n", "            stops += 1\n", "        else:\n", "            servers, time = result\n", "            results[servers].append(time)\n", "\n", "    return results\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "# Running the trials"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import multiprocessing as mp\n", "\n", "numbers_of_servers = [1, 2, 3]\n", "trials = range(100)\n", "workers = mp.cpu_count()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["workers\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_with_first_principles(numbers_of_servers, trials, workers):\n", "    \"\"\" Run all of the trials using first principles. \"\"\"\n", "\n", "    work_queue, done_queue = mp.Queue(), mp.Queue()\n", "\n", "    put_down_tasks(work_queue, numbers_of_servers, trials)\n", "    start_workers(work_queue, done_queue, workers)\n", "\n", "    results = process_done_queue(done_queue, workers)\n", "    return results\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "results = run_with_first_principles(numbers_of_servers, trials, workers)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "\n", "df = pd.DataFrame(results)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.describe()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "# Standard approach (letting `multiprocessing` do the work)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_with_starmap(args, num_cores=mp.cpu_count()):\n", "    \"\"\" Run all of the trials using `multiprocessing.Pool.starmap`. \"\"\"\n", "\n", "    with mp.Pool(num_cores) as pool:\n", "        raw_data = pool.starmap(run_simulation, args)\n", "\n", "    results = defaultdict(list)\n", "    for servers, time in raw_data:\n", "        results[servers].append(time)\n", "\n", "    return results\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "args = itertools.product(numbers_of_servers, trials)\n", "results = run_with_starmap(args)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "# Using `dask` to parallelise trials"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import dask\n", "\n", "\n", "def build_tasks(args):\n", "    \"\"\" Build all of the trials as tasks using `dask`. \"\"\"\n", "\n", "    tasks = [dask.delayed(run_simulation)(*arg) for arg in args]\n", "\n", "    return tasks\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["args = itertools.product(numbers_of_servers, trials)\n", "build_tasks(args)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def compute_tasks(tasks):\n", "    \"\"\" Execute the tasks and return results. \"\"\"\n", "\n", "    raw_data = dask.compute(*tasks, num_workers=mp.cpu_count(), scheduler=\"processes\")\n", "\n", "    results = defaultdict(list)\n", "    for servers, time in raw_data:\n", "        results[servers].append(time)\n", "\n", "    return results\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_with_dask(args):\n", "    \"\"\" Run all of the trials using `dask`. \"\"\"\n", "\n", "    tasks = build_tasks(args)\n", "    results = compute_tasks(tasks)\n", "\n", "    return results\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "args = itertools.product(numbers_of_servers, trials)\n", "results = run_with_dask(args)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "# What `dask` really is...\n", "\n", "<br>\n", "\n", "![Dask anatomy](img/collections-schedulers.png)\n", "\n", "<br>\n", "\n", "---\n", "# Custom parallelisation with `dask.delayed`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from time import sleep\n", "\n", "\n", "def inc(x):\n", "    sleep(0.5)\n", "    return x + 1\n", "\n", "\n", "def add(x, y):\n", "    sleep(0.5)\n", "    return x + y\n", "\n", "\n", "def mul(x, y):\n", "    sleep(0.5)\n", "    return x * y\n", "\n", "\n", "def exp(x, y):\n", "    sleep(0.5)\n", "    return x ** y\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "results = []\n", "a = inc(1)\n", "for i in range(5): \n", "    b = mul(i, a)\n", "    c = add(a, b)\n", "    d = exp(b, c)\n", "    results.append(d)\n", "\n", "total = sum(results)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["total\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@dask.delayed\n", "def inc(x):\n", "    sleep(0.5)\n", "    return x + 1\n", "\n", "\n", "@dask.delayed\n", "def add(x, y):\n", "    sleep(0.5)\n", "    return x + y\n", "\n", "\n", "@dask.delayed\n", "def mul(x, y):\n", "    sleep(0.5)\n", "    return x * y\n", "\n", "\n", "@dask.delayed\n", "def exp(x, y):\n", "    sleep(0.5)\n", "    return x ** y\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "results = []\n", "a = inc(1)\n", "for i in range(5): \n", "    b = mul(i, a)\n", "    c = add(a, b)\n", "    d = exp(b, c)\n", "    results.append(d)\n", "\n", "total = sum(results)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["total.visualize(rankdir=\"LR\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "total.compute()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "# `pandas`-esque data handling\n", "\n", "NYC flight data accessed at: [kaggle.com/usdot/flight-delays/version/1](https://www.kaggle.com/usdot/flight-delays/version/1#flights.csv)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import dask.dataframe as dd\n", "\n", "dtype = {\n", "    \"SCHEDULED_DEPARTURE\": object,\n", "    \"DEPARTURE_TIME\": object,\n", "    \"SCHEDULED_TIME\": float,\n", "    \"SCHEDULED_ARRIVAL\": object,\n", "    \"ARRIVAL_TIME\": object,\n", "}\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ddf = dd.read_csv(\"data/flights.csv\", dtype=dtype, low_memory=False)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nyc_codes = [\"JFK\", \"LGA\", \"EWR\"]\n", "nyc_departures = ddf[ddf[\"ORIGIN_AIRPORT\"].isin(nyc_codes)]\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mean_delay = nyc_departures.groupby(\"ORIGIN_AIRPORT\")[\"DEPARTURE_DELAY\"].mean()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mean_delay.visualize()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mean_delay.compute(num_workers=8, scheduler=\"processes\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "# Multi-dimensional arrays like `numpy`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import dask.array as da\n", "\n", "da.random.seed(0)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["array = da.random.normal(size=(1e4, 1e4))\n", "\n", "mean, std = array.mean(), array.std()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dask.visualize(*[mean, std])\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dask.compute([mean, std], num_workers=8, scheduler=\"processes\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["A = da.random.normal(size=(1e4, 1e4))\n", "B = da.random.normal(size=1e4)\n", "\n", "C = A.dot(B).sum() / A.mean() * B.var()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["C.visualize()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 2}